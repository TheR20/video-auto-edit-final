{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_descargado = \"entrev.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias\n",
    "import os\n",
    "import numpy as np\n",
    "import moviepy.editor as mp\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "import subprocess\n",
    "from TTS.api import TTS\n",
    "import torch\n",
    "from IPython.display import Audio, display\n",
    "import whisper\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from moviepy.editor import TextClip\n",
    "from moviepy.editor import CompositeVideoClip\n",
    "import soundfile as sf\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, TextClip, CompositeVideoClip\n",
    "import re\n",
    "from PIL import ImageFont\n",
    "\n",
    "\n",
    "ruta_carpeta_principal = \"C://Users//R20//Desktop//videostraduccidos//\"\n",
    "ruta_carpeta_audios_esp = \"audios_creados//audio_esp.wav\"\n",
    "ruta_carpeta_audios_ing = \"audios_creados//audio_ing.wav\"\n",
    "ruta_carpeta_audios_fin = \"audios_creados//audio_fin.wav\"\n",
    "ruta_carpeta_videofin = \"videos_creados//\" + nombre_descargado\n",
    "nombre_video = \"videos_descargados//\" + nombre_descargado\n",
    "\n",
    "rutaVideo = ruta_carpeta_principal + nombre_video\n",
    "rutaAudioIngles = ruta_carpeta_principal + ruta_carpeta_audios_ing\n",
    "rutaAudioGenerado =  ruta_carpeta_principal + ruta_carpeta_audios_esp\n",
    "rutaAudioGeneradoNuevaVelocidad = ruta_carpeta_principal + ruta_carpeta_audios_fin\n",
    "ruta_video_final = ruta_carpeta_principal + ruta_carpeta_videofin\n",
    "\n",
    "target_language = \"Spanish\" \n",
    "# Mapping between full names and ISO 639-1 codes\n",
    "language_mapping = {'English': 'en', 'Spanish': 'es', 'Chinese (Simplified)': 'zh-cn' , 'Japanese': 'ja' }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio text: ハーフとして日本で生活するってどんな感じだなかなと思っていてそうですねなんかまあ生まれもスタッチも日本で住んだことあるのも日本だけなんですけど見た目がやっぱり外国人なんでいくら頑張っても日本人にはなれないんですよ日本人であるためには日本語が完璧に喋れて日本の文化を理解して日本のなんか上式とか関心もわかっててなおかつ見た目も日本人じゃないとなかなか特に年上の世代の方とかから日本人として受けれてもらえないでも特数こともいっぱいあるんで難しいところだなと思いますアイデンティティに苦しむこう結構多いと思いますね特に小学生の頃とかは結構いろいろ自分は日本人じゃないのかはみたいなとかなること待っててまあ両方の世界に形すツッコメレっていうとっけんがあるなっていう風に今は思ってますなるほどね\n",
      "Translated text: Me pregunto cómo es vivir en Japón como una mitad, y parece que he vivido en Japón en Japón, pero es solo en Japón que he vivido en Japón, pero parece un extranjero, así que incluso si yo Haz lo mejor que pueda, será japonés. No puedo ser una persona japonesa, por lo que puedo hablar japonés perfectamente, entender la cultura japonesa, comprender el estilo de japonés e interés, y es un año bastante. son muchas cosas que no se pueden recibir como japoneses de la generación anterior. Creo que es bastante difícil sufrir de identidad. No sé si soy una persona, y estoy pensando que hay un tsukkomele Eso se puede moldear en ambos mundos.\n"
     ]
    }
   ],
   "source": [
    "#Generar texto del video\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(rutaVideo)\n",
    "\n",
    "whisper_text = result[\"text\"]\n",
    "whisper_language = result['language']\n",
    "\n",
    "print(\"Audio text:\", whisper_text)\n",
    "\n",
    "target_language_code = language_mapping[target_language]\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "translated_text = translator.translate(whisper_text, src=whisper_language, dest=target_language_code).text\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      "MoviePy - Writing audio in C://Users//R20//Desktop//videostraduccidos//audios_creados//audio_ing.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " > Text splitted to sentences.\n",
      "['Me pregunto cómo es vivir en Japón como una mitad, y parece que he vivido en Japón en Japón, pero es solo en Japón que he vivido en Japón, pero parece un extranjero, así que incluso si yo Haz lo mejor que pueda, será japonés.', 'No puedo ser una persona japonesa, por lo que puedo hablar japonés perfectamente, entender la cultura japonesa, comprender el estilo de japonés e interés, y es un año bastante.', 'son muchas cosas que no se pueden recibir como japoneses de la generación anterior.', 'Creo que es bastante difícil sufrir de identidad.', 'No sé si soy una persona, y estoy pensando que hay un tsukkomele Eso se puede moldear en ambos mundos.']\n",
      " > Processing time: 24.485398530960083\n",
      " > Real-time factor: 0.47283598193062937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C://Users//R20//Desktop//videostraduccidos//audios_creados//audio_esp.wav'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generar Audio\n",
    "\n",
    "# Confirmar la licencia\n",
    "confirmation = input(\"You must confirm the licensing terms. Do you agree? (y/n): \")\n",
    "if confirmation.lower() != 'y':\n",
    "    print(\"Exiting. License agreement not accepted.\")\n",
    "    exit()\n",
    "\n",
    "# Crear instancia de TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(\"cuda\")\n",
    "\n",
    "video = mp.VideoFileClip(rutaVideo)\n",
    "audio_clip = video.audio\n",
    "audio_clip.write_audiofile(rutaAudioIngles)\n",
    "\n",
    "# Generar audio\n",
    "tts.tts_to_file(\n",
    "    translated_text,\n",
    "    speaker_wav=rutaAudioIngles,\n",
    "    file_path=rutaAudioGenerado,\n",
    "    language=target_language_code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiar la velocidad del audio generado\n",
    "translated_text = translated_text.replace('.', '. ').replace('?', '? ')\n",
    "\n",
    "cambio_velocidad = 2\n",
    "velocidad_correcta = False\n",
    "y_original, sr_original = librosa.load(rutaAudioIngles)\n",
    "y_generated, sr_generated = librosa.load(rutaAudioGenerado)\n",
    "duration_original = len(y_original) / sr_original\n",
    "duration_original_esp = len(y_generated) / sr_generated\n",
    "print(f\"La duración del audio original es de {duration_original:.2f} segundos\")\n",
    "y_adjusted = y_original\n",
    "resta_originales = duration_original - duration_original_esp\n",
    "\n",
    "speed_ratio = cambio_velocidad \n",
    "sr_adjusted = int(sr_original * speed_ratio) \n",
    "y_adjusted = librosa.effects.time_stretch(y_generated, rate=speed_ratio)\n",
    "duration_adjusted = len(y_adjusted) / sr_adjusted\n",
    "resta = duration_original - duration_adjusted\n",
    "display(resta)\n",
    "\n",
    "''' Ajuste Automatico \n",
    "while(velocidad_correcta == False ): \n",
    "            display(\"Hola entre al while\")\n",
    "            speed_ratio = cambio_velocidad \n",
    "            sr_adjusted = int(sr_original * speed_ratio) \n",
    "            y_adjusted = librosa.effects.time_stretch(y_generated, rate=speed_ratio)\n",
    "            duration_adjusted = len(y_adjusted) / sr_adjusted\n",
    "            print(f\"La duración del audio ajustado es de {duration_adjusted:.2f} segundos\")\n",
    "            resta = duration_original - duration_adjusted\n",
    "            if -3.0 <= resta <= 3.0: # Si la diferencia entre las dos\n",
    "                  print(\"Ya sali del while\")\n",
    "                  print(resta)\n",
    "                  velocidad_correcta = True # Detener el bucle si la diferencia está dentro del rango permitido\n",
    "            elif duration_adjusted < duration_original:\n",
    "                  print(resta)\n",
    "                  cambio_velocidad -= .1  # Aumentamos el ratio\n",
    "            elif duration_adjusted > duration_original:\n",
    "                  print(resta)\n",
    "                  cambio_velocidad += .1\n",
    "            else:\n",
    "                  velocidad_correcta = True \n",
    "\n",
    "resta = duration_original - duration_adjusted '''\n",
    "\n",
    "sf.write(rutaAudioGeneradoNuevaVelocidad, y_adjusted, sr_original)\n",
    "# Mostrar el audio\n",
    "audio_widget = Audio(filename=rutaAudioGeneradoNuevaVelocidad, autoplay=False)\n",
    "display(audio_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generar letreros en el video.\n",
    "video = mp.VideoFileClip(rutaVideo)\n",
    "\n",
    "texto_que_cubre = \"Tengo un Bayblade \\n en mi bolsillo\"\n",
    "fontsize = 40\n",
    "duration = video.duration\n",
    "\n",
    "posicion = (\"center\", 220)\n",
    "subtitle = TextClip(texto_que_cubre, fontsize=fontsize,font=\"Bro Alex.ttf\", color='white', bg_color='black')\n",
    "subtitle_clip = subtitle.set_pos(posicion).set_duration(duration)\n",
    "#subtitle_clips.append(subtitle_clip)\n",
    "\n",
    "\n",
    "# Overlay the subtitle clips on the video clip\n",
    "video_with_subtitles = CompositeVideoClip([video, subtitle_clip])\n",
    "\n",
    "# Save the resulting clip\n",
    "video_with_subtitles.write_videofile(ruta_video_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poner subtitulos y audio nuevo al video.\n",
    "video = mp.VideoFileClip(rutaVideo)\n",
    "\n",
    "# Cargar el audio en español\n",
    "audio_spanish = mp.AudioFileClip(rutaAudioGeneradoNuevaVelocidad)\n",
    "\n",
    "# Split the translated text into words\n",
    "words = translated_text.split()\n",
    "\n",
    "# Group the words into pairs\n",
    "#subtitle_lines = [' '.join(words[i:i+2]) for i in range(0, len(words), 2)]\n",
    "subtitle_lines = []\n",
    "\n",
    "i = 0\n",
    "while i < len(words):\n",
    "    if i + 1 < len(words) and words[i+1].strip() != '':\n",
    "        # Si el siguiente elemento de la lista no está vacío, unir las dos palabras con un espacio\n",
    "        subtitle_lines.append(' '.join([words[i], words[i+1]]))\n",
    "        i += 2\n",
    "    else:\n",
    "        # Si el siguiente elemento de la lista está vacío o es el último elemento, agregar solo la palabra actual\n",
    "        subtitle_lines.append(words[i])\n",
    "        i += 1\n",
    "\n",
    "#for i in range(0, len(words), 2):\n",
    "    #if len(words[i]) > 8:\n",
    "     #   subtitle_lines.append(words[i])\n",
    "      #  subtitle_lines.append(words[i+1])\n",
    "    #else:\n",
    "     #   subtitle_lines.append(' '.join([words[i], words[i+1]]))\n",
    "\n",
    "# Remove any empty strings from the list of subtitle lines\n",
    "subtitle_lines = [line.strip() for line in subtitle_lines if line.strip()]\n",
    "\n",
    "# Create a list to store the text clips\n",
    "subtitle_clips = []\n",
    "\n",
    "# Set the font size and duration for each subtitle line\n",
    "fontsize = 45\n",
    "# Register the font\n",
    "#font = ImageFont.truetype(\"Bubblegum.ttf\", fontsize)\n",
    "duration = video.duration / len(subtitle_lines)\n",
    "\n",
    "# Initialize a variable to keep track of the current line number\n",
    "line_number = 0\n",
    "\n",
    "# Create a TextClip for each subtitle line and add it to the list\n",
    "for i, line in enumerate(subtitle_lines):\n",
    "    line = line.upper()\n",
    "    if line_number % 3 == 0:\n",
    "        color = 'red'\n",
    "    elif line_number % 3 == 1:\n",
    "        color = 'yellow'\n",
    "    else:\n",
    "        color = 'white'\n",
    "\n",
    "    posicion = (\"center\", 750)\n",
    "    subtitle = TextClip(line, fontsize=fontsize,font=\"Super Disco Personal Use.ttf\", color=color, bg_color='black')\n",
    "    subtitle_clip = subtitle.set_pos(posicion).set_duration(duration).set_start(i * duration)\n",
    "    subtitle_clips.append(subtitle_clip)\n",
    "\n",
    "    # Increment the line number\n",
    "    line_number += 1\n",
    "\n",
    "# Overlay the subtitle clips on the video clip\n",
    "video_with_subtitles = CompositeVideoClip([video] + subtitle_clips)\n",
    "\n",
    "# Set the audio of the composite clip\n",
    "video_with_subtitles.audio = audio_spanish\n",
    "\n",
    "# Save the resulting clip\n",
    "video_with_subtitles.write_videofile(ruta_video_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generar letrero automaticamente v1\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip, CompositeVideoClip, TextClip\n",
    "from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "# Traducir el texto directamente\n",
    "from googletrans import Translator\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C://Program Files//Tesseract-OCR//tesseract.exe'\n",
    "\n",
    "\n",
    "# Inicializar la captura de video\n",
    "cap = cv2.VideoCapture(rutaVideo)\n",
    "video = mp.VideoFileClip(rutaVideo)\n",
    "\n",
    "# Saltar a la posición del texto original (frame 10)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 10)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Si no se pudo leer el frame, salir\n",
    "if not ret:\n",
    "    print(\"Error: No se pudo leer el frame.\")\n",
    "    exit()\n",
    "\n",
    "# Convertir el frame a escala de grises\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Usar pytesseract para detectar el texto original\n",
    "texto_original = pytesseract.image_to_string(gray)\n",
    "\n",
    "# Obtener la plantilla a partir del texto original\n",
    "x, y, w, h = cv2.boundingRect(gray)\n",
    "template = gray[y:y+h, x:x+w]\n",
    "\n",
    "# Extraer la información del video\n",
    "duration = video.duration\n",
    "fps = video.fps\n",
    "\n",
    "# Ajustar el tamaño de la plantilla (opcional)\n",
    "template = cv2.resize(template, (int(w*1.5), int(h*1.5)))\n",
    "\n",
    "# Almacenar el texto original en una variable\n",
    "texto_original = pytesseract.image_to_string(gray)\n",
    "\n",
    "# Refinar la posición del texto original\n",
    "contours, _ = cv2.findContours(template, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "x_original, y_original, w_original, h_original = cv2.boundingRect(contours[0])\n",
    "\n",
    "# Variables de ajuste de posición (en píxeles)\n",
    "offset_x = 0  # Ajuste horizontal (desplazamiento a la derecha)\n",
    "offset_y = 0  # Ajuste vertical (desplazamiento hacia abajo)\n",
    "\n",
    "# Calcular la posición del nuevo texto\n",
    "text_size = cv2.getTextSize(texto_que_cubre, font, fontsize, 2)[0]\n",
    "x_nuevo = int(x_original + (w_original - text_size[0]) / 2 + offset_x)\n",
    "y_nuevo = int(y_original + (h_original + text_size[1]) / 2 + offset_y)\n",
    "\n",
    "\n",
    "\n",
    "# Instanciar el traductor\n",
    "translator = Translator()\n",
    "\n",
    "# Traducir el texto\n",
    "translated_text = translator.translate(texto_original, dest=\"es\").text\n",
    "\n",
    "# Definir el nuevo texto a partir de la traducción\n",
    "texto_que_cubre = translated_text\n",
    "\n",
    "# Definir el tamaño de la fuente y el color\n",
    "fontsize = 20\n",
    "color = 'white'\n",
    "\n",
    "# Crear un clip de texto con el nuevo texto\n",
    "subtitle = TextClip(texto_que_cubre, fontsize=fontsize, font=\"Bro Alex.ttf\", color=color, bg_color='black')\n",
    "\n",
    "# Calcular la posición del nuevo texto\n",
    "x_template, y_template = template.shape[:2]\n",
    "x_nuevo = int((video.w - x_template) / 2)\n",
    "y_nuevo = int((video.h - y_template) /2)\n",
    "\n",
    "# Crear un clip de texto con el nuevo texto\n",
    "subtitle = TextClip(texto_que_cubre, fontsize=fontsize, font=\"Bro Alex.ttf\", color=color, bg_color='black')\n",
    "\n",
    "# Posicionar el clip de texto sobre el original\n",
    "subtitle_clip = subtitle.set_pos((x_nuevo, y_nuevo)).set_duration(duration)\n",
    "\n",
    "# Combinar el video original con el clip de texto\n",
    "video_con_subtitulos = CompositeVideoClip([video, subtitle_clip])\n",
    "\n",
    "# Renderizar y guardar el video final\n",
    "video_con_subtitulos.write_videofile(ruta_video_final, fps=fps)\n",
    "\n",
    "# Liberar la captura de video\n",
    "cap.release()\n",
    "\n",
    "# Mostrar un mensaje de éxito\n",
    "print(\"Video con subtítulos creado:\", ruta_video_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generar letrero automaticamente v2\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip, CompositeVideoClip, TextClip\n",
    "from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "\n",
    "# Traducir el texto directamente\n",
    "from googletrans import Translator\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C://Program Files//Tesseract-OCR//tesseract.exe'\n",
    "\n",
    "\n",
    "# Inicializar la captura de video\n",
    "cap = cv2.VideoCapture(rutaVideo)\n",
    "video = mp.VideoFileClip(rutaVideo)\n",
    "\n",
    "# Saltar a la posición del texto original (frame 10)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 10)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Si no se pudo leer el frame, salir\n",
    "if not ret:\n",
    "    print(\"Error: No se pudo leer el frame.\")\n",
    "    exit()\n",
    "\n",
    "# Convertir el frame a escala de grises\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Usar pytesseract para detectar el texto original\n",
    "texto_original = pytesseract.image_to_string(gray)\n",
    "\n",
    "# Obtener la plantilla a partir del texto original\n",
    "x, y, w, h = cv2.boundingRect(gray)\n",
    "template = gray[y:y+h, x:x+w]\n",
    "\n",
    "# Extraer la información del video\n",
    "duration = video.duration\n",
    "fps = video.fps\n",
    "\n",
    "# Ajustar el tamaño de la plantilla (opcional)\n",
    "template = cv2.resize(template, (int(w*1.5), int(h*1.5)))\n",
    "\n",
    "# Almacenar el texto original en una variable\n",
    "texto_original = pytesseract.image_to_string(gray)\n",
    "\n",
    "# Refinar la posición del texto original\n",
    "contours, _ = cv2.findContours(template, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "x_original, y_original, w_original, h_original = cv2.boundingRect(contours[0])\n",
    "\n",
    "# Instanciar el traductor\n",
    "translator = Translator()\n",
    "\n",
    "# Traducir el texto\n",
    "translated_text = translator.translate(texto_original, dest=\"es\").text\n",
    "\n",
    "# Definir el nuevo texto a partir de la traducción\n",
    "texto_que_cubre = translated_text\n",
    "\n",
    "# Definir el tamaño de la fuente y el color\n",
    "fontsize = 18\n",
    "color = 'white'\n",
    "\n",
    "# Obtener el tamaño del nuevo texto\n",
    "text_size = cv2.getTextSize(texto_que_cubre, font, fontsize, 2)[0]\n",
    "\n",
    "# Calcular la posición central del nuevo texto\n",
    "x_nuevo = int(x_original + (w_original - text_size[0]) / 2)\n",
    "y_nuevo = int(y_original + (h_original + text_size[1]) / 2)\n",
    "\n",
    "# Crear un clip de texto con el nuevo texto\n",
    "subtitle = TextClip(texto_que_cubre, fontsize=fontsize, font=\"Bro Alex.ttf\", color=color, bg_color='black')\n",
    "\n",
    "\n",
    "display(x_original)\n",
    "display(x_nuevo)\n",
    "display(y_original)\n",
    "display(y_nuevo)\n",
    "\n",
    "# Posicionar el clip de texto sobre el original\n",
    "subtitle_clip = subtitle.set_pos((\"center\",100)).set_duration(duration)\n",
    "\n",
    "# Combinar el video original con el clip de texto\n",
    "video_con_subtitulos = CompositeVideoClip([video, subtitle_clip])\n",
    "\n",
    "# Renderizar y guardar el video final\n",
    "video_con_subtitulos.write_videofile(ruta_video_final, fps=fps)\n",
    "\n",
    "# Liberar la captura de video\n",
    "cap.release()\n",
    "\n",
    "# Mostrar un mensaje de éxito\n",
    "print(\"Video con subtítulos creado:\", ruta_video_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
